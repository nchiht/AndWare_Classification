{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKU5Rn9etS7_"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/intel/openfl.git\n",
        "!pip install -r requirements_workflow_interface.txt\n",
        "\n",
        "# Uncomment this if running in Google Colab\n",
        "!pip install -r https://raw.githubusercontent.com/intel/openfl/develop/openfl-tutorials/experimental/requirements_workflow_interface.txt\n",
        "import os\n",
        "os.environ[\"USERNAME\"] = \"colab\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "\n",
        "n_epochs = 3\n",
        "batch_size_train = 64\n",
        "batch_size_test = 1000\n",
        "learning_rate = 0.01\n",
        "momentum = 0.5\n",
        "log_interval = 10\n",
        "\n",
        "random_seed = 1\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "mnist_train = torchvision.datasets.MNIST('files/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ]))\n",
        "\n",
        "mnist_test = torchvision.datasets.MNIST('files/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ]))\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x)\n",
        "\n",
        "def inference(network,test_loader):\n",
        "    network.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "      for data, target in test_loader:\n",
        "        output = network(data)\n",
        "        test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "      test_loss, correct, len(test_loader.dataset),\n",
        "      100. * correct / len(test_loader.dataset)))\n",
        "    accuracy = float(correct / len(test_loader.dataset))\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "lOQLr6Hsz5FO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "from openfl.experimental.interface import FLSpec, Aggregator, Collaborator\n",
        "from openfl.experimental.runtime import LocalRuntime\n",
        "from openfl.experimental.placement import aggregator, collaborator\n",
        "\n",
        "\n",
        "def FedAvg(models, weights=None):\n",
        "    new_model = models[0]\n",
        "    state_dicts = [model.state_dict() for model in models]\n",
        "    state_dict = new_model.state_dict()\n",
        "    for key in models[1].state_dict():\n",
        "        state_dict[key] = torch.from_numpy(np.average([state[key].numpy() for state in state_dicts],\n",
        "                                                      axis=0,\n",
        "                                                      weights=weights))\n",
        "    new_model.load_state_dict(state_dict)\n",
        "    return new_model"
      ],
      "metadata": {
        "id": "ShvYJAAVz9xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FederatedFlow(FLSpec):\n",
        "\n",
        "    def __init__(self, model = None, optimizer = None, rounds=3, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        if model is not None:\n",
        "            self.model = model\n",
        "            self.optimizer = optimizer\n",
        "        else:\n",
        "            self.model = Net()\n",
        "            self.optimizer = optim.SGD(self.model.parameters(), lr=learning_rate,\n",
        "                                   momentum=momentum)\n",
        "        self.rounds = rounds\n",
        "\n",
        "    @aggregator\n",
        "    def start(self):\n",
        "        print(f'Performing initialization for model')\n",
        "        self.collaborators = self.runtime.collaborators\n",
        "        self.private = 10\n",
        "        self.current_round = 0\n",
        "        self.next(self.aggregated_model_validation,foreach='collaborators',exclude=['private'])\n",
        "\n",
        "    @collaborator\n",
        "    def aggregated_model_validation(self):\n",
        "        print(f'Performing aggregated model validation for collaborator {self.input}')\n",
        "        self.agg_validation_score = inference(self.model,self.test_loader)\n",
        "        print(f'{self.input} value of {self.agg_validation_score}')\n",
        "        self.next(self.train)\n",
        "\n",
        "    @collaborator\n",
        "    def train(self):\n",
        "        self.model.train()\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=learning_rate,\n",
        "                                   momentum=momentum)\n",
        "        train_losses = []\n",
        "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
        "          self.optimizer.zero_grad()\n",
        "          output = self.model(data)\n",
        "          loss = F.nll_loss(output, target)\n",
        "          loss.backward()\n",
        "          self.optimizer.step()\n",
        "          if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: 1 [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "               batch_idx * len(data), len(self.train_loader.dataset),\n",
        "              100. * batch_idx / len(self.train_loader), loss.item()))\n",
        "            self.loss = loss.item()\n",
        "            torch.save(self.model.state_dict(), 'model.pth')\n",
        "            torch.save(self.optimizer.state_dict(), 'optimizer.pth')\n",
        "        self.training_completed = True\n",
        "        self.next(self.local_model_validation)\n",
        "\n",
        "    @collaborator\n",
        "    def local_model_validation(self):\n",
        "        self.local_validation_score = inference(self.model,self.test_loader)\n",
        "        print(f'Doing local model validation for collaborator {self.input}: {self.local_validation_score}')\n",
        "        self.next(self.join, exclude=['training_completed'])\n",
        "\n",
        "    @aggregator\n",
        "    def join(self,inputs):\n",
        "        self.average_loss = sum(input.loss for input in inputs)/len(inputs)\n",
        "        self.aggregated_model_accuracy = sum(input.agg_validation_score for input in inputs)/len(inputs)\n",
        "        self.local_model_accuracy = sum(input.local_validation_score for input in inputs)/len(inputs)\n",
        "        print(f'Average aggregated model validation values = {self.aggregated_model_accuracy}')\n",
        "        print(f'Average training loss = {self.average_loss}')\n",
        "        print(f'Average local model validation values = {self.local_model_accuracy}')\n",
        "        self.model = FedAvg([input.model for input in inputs])\n",
        "        self.optimizer = [input.optimizer for input in inputs][0]\n",
        "        self.current_round += 1\n",
        "        if self.current_round < self.rounds:\n",
        "            self.next(self.aggregated_model_validation, foreach='collaborators', exclude=['private'])\n",
        "        else:\n",
        "            self.next(self.end)\n",
        "\n",
        "    @aggregator\n",
        "    def end(self):\n",
        "        print(f'This is the end of the flow')"
      ],
      "metadata": {
        "id": "fROWYmrJ4g7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup participants\n",
        "aggregator = Aggregator()\n",
        "aggregator.private_attributes = {}\n",
        "\n",
        "# Setup collaborators with private attributes\n",
        "collaborator_names = ['Portland', 'Seattle', 'Chandler','Bangalore']\n",
        "collaborators = [Collaborator(name=name) for name in collaborator_names]\n",
        "for idx, collaborator in enumerate(collaborators):\n",
        "    local_train = deepcopy(mnist_train)\n",
        "    local_test = deepcopy(mnist_test)\n",
        "    local_train.data = mnist_train.data[idx::len(collaborators)]\n",
        "    local_train.targets = mnist_train.targets[idx::len(collaborators)]\n",
        "    local_test.data = mnist_test.data[idx::len(collaborators)]\n",
        "    local_test.targets = mnist_test.targets[idx::len(collaborators)]\n",
        "    collaborator.private_attributes = {\n",
        "            'train_loader': torch.utils.data.DataLoader(local_train,batch_size=batch_size_train, shuffle=True),\n",
        "            'test_loader': torch.utils.data.DataLoader(local_test,batch_size=batch_size_train, shuffle=True)\n",
        "    }\n",
        "\n",
        "local_runtime = LocalRuntime(aggregator=aggregator, collaborators=collaborators, backend='single_process')\n",
        "print(f'Local runtime collaborators = {local_runtime.collaborators}')"
      ],
      "metadata": {
        "id": "kmohxazF4keT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = None\n",
        "best_model = None\n",
        "optimizer = None\n",
        "flflow = FederatedFlow(model,optimizer)\n",
        "flflow.runtime = local_runtime\n",
        "flflow.run()"
      ],
      "metadata": {
        "id": "VIpsvZUN4w3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Sample of the final model weights: {flflow.model.state_dict()[\"conv1.weight\"][0]}')\n",
        "\n",
        "print(f'\\nFinal aggregated model accuracy for {flflow.rounds} rounds of training: {flflow.aggregated_model_accuracy}')"
      ],
      "metadata": {
        "id": "bF-6EbdH4xma"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}